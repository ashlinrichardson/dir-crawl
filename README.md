# dir-crawl

1) run crawl.py
2) use extract.py to convert the scraped data to a "comma separated value" (CSV) file

Note: the parse_html.py is a simple example of parsing an HTML file (extracts HREF tags).. this is fleshed out to extract more complicated fields (or series of fields)

Note: line 78 file 1) function wget(url): this is what actually pulls data from a specific address on web (i.e., a web page) in analogy to the linux/ unix command line utility "wget" ("web get")
